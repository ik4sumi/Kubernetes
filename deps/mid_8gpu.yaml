apiVersion: apps/v1
kind: Deployment
metadata:
  name: mid-8gpu
  labels:
    k8s-app: mid-8gpu
spec:
  replicas: 1
  selector:
    matchLabels:
      k8s-app: mid-8gpu
  template:
    metadata: 
      labels:
        k8s-app: mid-8gpu
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: nvidia.com/gpu.product
                operator: In
                values:
                - NVIDIA-GeForce-GTX-2080-Ti
                - NVIDIA-TITAN-Xp
                - NVIDIA-A100-PCIE-40GB-MIG-2g.10gb	
                - NVIDIA-GeForce-GTX-1080-Ti
                - Tesla-T4
      containers:
      - name: mid-8gpu
        image: pytorch/pytorch:2.0.0-cuda11.7-cudnn8-devel
        command: ["sh", "-c", "apt update && \
          apt install -y curl && \
          curl -fsSL https://raw.githubusercontent.com/miracleyoo/one-key-linux-setup/master/one-key-setup-for-pytorch-docker.sh -o one-key-setup-for-pytorch-docker.sh && \
          bash one-key-setup-for-pytorch-docker.sh && \
          rm one-key-setup-for-pytorch-docker.sh && \
          sleep infinity"]
        resources:
          limits:
            memory: 32Gi
            cpu: 16
            nvidia.com/gpu: 8
            ephemeral-storage: 100Gi
          requests:
            memory: 32Gi
            cpu: 4
            nvidia.com/gpu: 8
            ephemeral-storage: 100Gi        
        volumeMounts:
        - mountPath: /tsukimi
          name: tsukimi
        # - mountPath: /hataraku
        #   name: hataraku
        - mountPath: /dev/shm
          name: cache-volume
      volumes:
        - name: tsukimi
          persistentVolumeClaim:
            claimName: tsukimi
        # - name: hataraku
        #   persistentVolumeClaim:
        #     claimName: hataraku
        - emptyDir:
            medium: Memory
            sizeLimit: 1Gi
          name: cache-volume